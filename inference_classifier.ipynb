import pickle
import cv2
import mediapipe as mp
import numpy as np

# Load model and label map
model_dict = pickle.load(open('./model.p', 'rb'))
model = model_dict['model']
label_map = model_dict['label_map']
labels_dict = {v: k for k, v in label_map.items()}

# Initialize MediaPipe Hands
cap = cv2.VideoCapture(1)  # Change to 0 if camera index 1 doesn't work
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.3)

# Define expected feature length
expected_features_one_hand = 42
expected_features_two_hands = 84

while True:
    # Capture frame from video feed
    ret, frame = cap.read()
    if not ret:
        print("Failed to capture frame. Exiting...")
        break

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(frame_rgb)

    # Initialize auxiliary data for prediction
    data_aux = []

    if results.multi_hand_landmarks:
        hands_data = []
        for hand_landmarks in results.multi_hand_landmarks[:2]:
            hand_data = []
            x_hand = []
            y_hand = []

            # Extract x, y coordinates
            for lm in hand_landmarks.landmark:
                x_hand.append(lm.x)
                y_hand.append(lm.y)

            # Normalize coordinates
            for lm in hand_landmarks.landmark:
                hand_data.append(lm.x - min(x_hand))
                hand_data.append(lm.y - min(y_hand))

            hands_data.append(hand_data[:expected_features_one_hand])

        # Combine hand data into a single vector
        if len(hands_data) == 1:
            data_aux = hands_data[0] + [0] * expected_features_one_hand
        elif len(hands_data) == 2:
            data_aux = hands_data[0] + hands_data[1]

        # Ensure vector matches expected length
        if len(data_aux) == expected_features_two_hands:
            try:
                # Predict the character
                prediction = model.predict([np.asarray(data_aux)])
                predicted_character = labels_dict[int(prediction[0])]

                # Display the predicted character on the frame
                cv2.putText(frame, predicted_character, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (255, 0, 0), 3, cv2.LINE_AA)
            except Exception as e:
                print(f"Prediction error: {e}")

    # Show the frame
    cv2.imshow('Gesture Prediction', frame)

    # Exit on pressing 'q'
    if cv2.waitKey(10) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
